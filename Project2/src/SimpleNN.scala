
//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** @author  John Miller
 *  @version 2.0
 *  @date    Mon Oct 20 20:16:25 EDT 2025
 *  @see     LICENSE (MIT style license file).
 *
 *  @note    Simple Neural Networks Using Gradient Descent Optimization
 *           Tests both Gradient Descent (GD) and Incremental Gradient Descent (IGD)
 *           Simplified versions of `Regression`, `Perceptron`, `NeuralNet_2L`, and `NeuralNet_3L`
 *           for illustration/learning, not production
 *
 *  @note    the symbol ∆í indicates the derivative of function f, i.e., ∆í = f'
 */

package scalation.modeling

import scalation.?                                              // wildcard: xy(?, 3) gives column 3
import scalation.mathstat.{VectorD, MatrixD, Plot}
import scalation.mathstat.VectorDOps._
import scalation.modeling.ActivationFun.{f_sigmoid, sigmoid_}   // sigmoid activation functions

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `SimpleNN` object contains a simple dataset for testing Gradient Descent (GD)
 *  and Incremental Gradient Descent (IGD) optimization algorithms.
 *  @see https://nowak.ece.wisc.edu/MFML.pdf
 */
object SimpleNN:

    // 9 data points:         One    x1    x2    y1   y2
    val xy = MatrixD ((9, 5), 1.0,  0.1,  0.1,  0.5, 0.25,      // dataset
                              1.0,  0.1,  0.5,  0.3, 0.49,
                              1.0,  0.1,  1.0,  0.2, 0.64,

                              1.0,  0.5,  0.1,  0.8, 0.04,
                              1.0,  0.5,  0.5,  0.5, 0.25,
                              1.0,  0.5,  1.0,  0.3, 0.49,

                              1.0,  1.0,  0.1,  1.0, 0.0,
                              1.0,  1.0,  0.5,  0.8, 0.04,
                              1.0,  1.0,  1.0,  0.5, 0.25)
end SimpleNN

import SimpleNN.xy

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleNN1` main function illustrates the use of Gradient Descent (GD) to
 *  optimize the weights/parameters of a Multiple Linear Regression (MLR) model.
 *  Computations done at the vector level: X -> y.  R^2 = .827
 *  > runMain scalation.modeling.simpleNN1
 */
@main def simpleNN1 (): Unit =

    val (x, y) = (xy(?, 0 until 3), xy(?, 3))                   // input matrix, output/response vector
    val sst    = (y - y.mean).normSq                            // sum of squares total
    val b      = VectorD (0.1, 0.2, 0.1)                        // initial weights/parameters (random in practice)

    val Œ∑ = 0.1                                                 // learning rate (to be tuned)
    var ≈∑, Œµ, Œ¥, g: VectorD = null

    for epoch <- 1 to 10 do
        println (s"Improvement step $epoch")

        // forward prop: input -> output
        ≈∑  = x * b                                              // prediction vector
        Œµ  = y - ≈∑                                              // error vector

        // backward prop: output -> input
        Œ¥  = -Œµ                                                 // delta correction vector
        g  = x.ùêì * Œ¥                                            // gradient vector

        // parameter update
        b -= g * Œ∑                                              // update parameter vector
        val sse = Œµ.normSq                                      // sum of squared errors
        val r2  = 1.0 - sse / sst                               // R^2

        println (s"""
        ≈∑  = $≈∑
        Œµ  = $Œµ
        Œ¥  = $Œ¥
        g  = $g
        b  = $b
        r2 = $r2
        """)
    end for

    new Plot (null, y, ≈∑, "IGD for MLR y", lines = true)

end simpleNN1


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleNN2` main function illustrates the use of Incremental Gradient Descent (IGD)
 *  to optimize the weights/parameters of a Multiple Linear Regression (MLR) model.
 *  Computations done at the vector level: X -> y.  R^2 = .933
 *  > runMain scalation.modeling.simpleNN2
 */
@main def simpleNN2 (): Unit =

    val (xx, yy) = (xy(?, 0 until 3), xy(?, 3))                 // input matrix, output/response vector
    val sst    = (yy - yy.mean).normSq                          // sum of squares total
    val b      = VectorD (0.1, 0.2, 0.1)                        // initial weights/parameters (random in practice)

    val Œ∑ = 0.2                                                 // learning rate (to be tuned)
    var ≈∑, Œµ, Œ¥: Double = -0.0
    var g: VectorD = null
    val yp = new VectorD (yy.dim)                               // save each prediction in yp

    for epoch <- 1 to 10 do
        println (s"Improvement step $epoch")
        var sse = 0.0
        for i <- xx.indices do
            val (x, y) = (xx(i), yy(i))                         // randomize i for Stochastic Gradient Descent (SGD)

            // forward prop: input -> output
            ≈∑  = x ‚àô b                                          // prediction scalar
            Œµ  = y - ≈∑                                          // error scalar

            // backward prop: output -> input
            Œ¥  = -Œµ                                             // delta correction scalar
            g  = x * Œ¥                                          // gradient vector

            // parameter update
            b -= g * Œ∑                                          // update parameter vector
            yp(i) = ≈∑                                           // save i-th prediction
            sse += Œµ * Œµ                                        // sum of squared errors
        end for
        val r2 = 1.0 - sse / sst                                // R^2

        println (s"""
        ≈∑  = $≈∑
        Œµ  = $Œµ
        Œ¥  = $Œ¥
        g  = $g
        b  = $b
        r2 = $r2
        """)
    end for

    new Plot (null, yy, yp, "IGD for MLR y", lines = true)

end simpleNN2


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleNN3` main function illustrates the use of Gradient Descent (GD) to
 *  optimize the weights/parameters of a simple neural network (Perceptron).
 *  Originally, perceptrons used the Heavyside activation function for binary classification
 *  problems, but have been extended to multi-class classification and regression problems.
 *  Furthermore, when the activation function is identity, the perceptron models are equivalent
 *  multiple linear regression models.
 *  Computations done at the vector level: X -> y.  R^2 = .865
 *  > runMain scalation.modeling.simpleNN3
 */
@main def simpleNN3 (): Unit =

    val (x, y) = (xy(?, 0 until 3), xy(?, 3))                   // input matrix, output/response vector
    val sst    = (y - y.mean).normSq                            // sum of squares total
    val b      = VectorD (0.1, 0.2, 0.1)                        // initial weights/parameters (random in practice)

    val Œ∑ = 2.5                                                 // learning rate (to be tuned)
    var u, ≈∑, Œµ, ∆í, Œ¥, g: VectorD = null

    for epoch <- 1 to 10 do
        println (s"Improvement step $epoch")

        // forward prop: input -> output
        u  = x * b                                              // pre-activation vector
        ≈∑  = sigmoid_ (u)                                       // prediction vector
        Œµ  = y - ≈∑                                              // error vector

        // backward prop: output -> input
        ∆í  = ≈∑ * (1.0 - ≈∑)                                      // derivative (f') for sigmoid
        Œ¥  = -Œµ * ∆í                                             // delta correction vector
        g  = x.ùêì * Œ¥                                            // gradient vector

        // parameter update
        b -= g * Œ∑                                              // update parameter vector
        val sse = Œµ.normSq                                      // sum of squared errors
        val r2  = 1.0 - sse / sst                               // R^2

        println (s"""
        u  = $u
        ≈∑  = $≈∑ 
        Œµ  = $Œµ
        ∆í  = $∆í
        Œ¥  = $Œ¥
        g  = $g
        b  = $b
        r2 = $r2
        """)

    end for
    new Plot (null, y, ≈∑, "GD for Perceptron y", lines = true)

end simpleNN3


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleNN4` main function illustrates the use of Gradient Descent (GD) to optimize
 *  the weights/parameters of a simple neural network (2-layer (no hidden) Neural Network).
 *
 *  Prediction Equation: ≈∑ = f(B^T x) = f(W x + b) 
 *
 *  where x is an input vector, ≈∑ is a predicted output vector, f is the activation function,
 *  B is the parameter matrix, W is the weight matrix, and b is the bias vector.
 *  @note, PyTorch stores the weight matrix W [num_out, num_in] transposed from B to make
 *  back-propagation more efficient.
 *  Computations done at the vector level: X -> y.  R^2 = .865, .826
 *  > runMain scalation.modeling.simpleNN4
 */
@main def simpleNN4 (): Unit =

    val (x, y) = (xy(?, 0 until 3), xy(?, 3 until 5))           // input matrix, output/response matrix
    val sst    = (y - y.mean).normSq                            // sum of squares total, per column
    val b      = MatrixD ((3, 2), 0.1, 0.1,
                                  0.2, 0.1,
                                  0.1, 0.1)                     // initial weights/parameters (random in practice)

    val Œ∑ = 2.5                                                 // learning rate (to be tuned)
    var u, ≈∑, Œµ, ∆í, Œ¥, g: VectorD = null

    for epoch <- 1 to 10; k <- y.indices2 do
        println (s"Improvement step $epoch")

        // forward prop: input -> output
        u  = x * b(?, k)                                        // pre-activation vector
        ≈∑  = sigmoid_ (u)                                       // prediction vector
        Œµ  = y(?, k) - ≈∑                                        // error vector for column k

        // backward prop: output -> input
        ∆í  = ≈∑ * (1.0 - ≈∑)                                      // derivative (f') for sigmoid
        Œ¥  = -Œµ * ∆í                                             // delta correction vector
        g  = x.ùêì * Œ¥                                            // gradient vector

        // parameter update
        b(?, k) = b(?, k) - g * Œ∑                               // update parameter matrix, column k
        val sse = Œµ.normSq                                      // sum of squared errors
        val r2  = 1.0 - sse / sst(k)                            // R^2

        println (s"""
        for k = $k
        u  = $u
        ≈∑  = $≈∑
        Œµ  = $Œµ
        ∆í  = $∆í
        Œ¥  = $Œ¥
        g  = $g
        b  = $b
        r2 = $r2
        """)
    end for

    new Plot (null, y(?, 1), ≈∑, "GD for Two-layer Neural Net y_1", lines = true)

end simpleNN4


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleNN5` main function illustrates the use of Gradient Descent (GD) to optimize
 *  the weights/parameters of a simple neural network (2-layer (no hidden) Neural Network).
 *  Computations done at the matrix level: X -> Y.  R^2 = .865, .826
 *  > runMain scalation.modeling.simpleNN5
 */
@main def simpleNN5 (): Unit =

    import scalation.mathstat.MatrixDOps._                      // may clash with VectorDOps

    val (x, y) = (xy(?, 0 until 3), xy(?, 3 until 5))           // input matrix, output/response matrix
    val sst    = (y - y.mean).normSq                            // sum of squares total, per column
    val b      = MatrixD ((3, 2), 0.1, 0.1,
                                  0.2, 0.1,
                                  0.1, 0.1)                     // initial weights/parameters (random in practice)

    val Œ∑ = 2.5                                                 // learning rate (to be tuned)
    var u, ≈∑, Œµ, ∆í, Œ¥, g: MatrixD = null

    for epoch <- 1 to 10 do
        println (s"Improvement step $epoch")

        // forward prop: input -> output
        u  = x * b                                              // pre-activation matrix
        ≈∑  = f_sigmoid.fM (u)                                   // prediction matrix
        Œµ  = y - ≈∑                                              // error matrix

        // backward prop: output -> input
        ∆í  = ≈∑ ‚äô (1.0 - ≈∑)                                      // derivative (f') for sigmoid
        Œ¥  = -Œµ ‚äô ∆í                                             // delta correction matrix via Hadamard product
        g  = x.ùêì * Œ¥                                            // transposed Jacobian matrix (gradients)

        // parameter update
        b -= g * Œ∑                                              // update parameter matrix
        val sse = Œµ.normSq                                      // sum of squared errors
        val r2  = -(sse / sst - 1.0)                            // R^2 (recoded to avoid Ops clash)

        println (s"""
        u  = $u
        ≈∑  = $≈∑
        Œµ  = $Œµ
        ∆í  = $∆í
        Œ¥  = $Œ¥
        g  = $g
        b  = $b
        r2 = $r2
        """)
    end for

    new Plot (null, y(?, 0), ≈∑(?, 0), "GD for Two-layer Neural Net y_0", lines = true)
    new Plot (null, y(?, 1), ≈∑(?, 1), "GD for Two-layer Neural Net y_1", lines = true)

end simpleNN5


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleNN6` main function illustrates the use of Gradient Descent (GD) to optimize
 *  the weights/parameters of a simple neural network (3-layer (1 hidden) Neural Network).
 *  Computations done at the matrix level: X -> Z -> Y.  R^2 = .299, .432 (requires more epochs)
 *  > runMain scalation.modeling.simpleNN6
 */
@main def simpleNN6 (): Unit =

    import scalation.mathstat.MatrixDOps._                      // may clash with VectorDOps

    val (x, y) = (xy(?, 1 until 3), xy(?, 3 until 5))           // input matrix, output/response matrix
    val sst    = (y - y.mean).normSq                            // sum of squares total, per column
    val a      = MatrixD ((2, 3), 0.1, 0.1, 0.1,                // parameter/weight matrix: input -> hidden
                                  0.1, 0.1, 0.1)
    val Œ±      = VectorD (0.1, 0.1, 0.1)                        // hidden layer bias vector
    val b      = MatrixD ((3, 2), 0.1, 0.1,                     // parameter/weight matrix: hidden -> output
                                  0.2, 0.1,
                                  0.1, 0.1)                     // initial weights/parameters (random in practice)
    val Œ≤      = VectorD (0.1, 0.1)                             // output layer bias vector

    val Œ∑ = 10.0                                                // learning rate (to be tuned)
    var u, z, v, ≈∑, Œµ, ∆í1, Œ¥1, g1, ∆í0, Œ¥0, g0: MatrixD = null

    for epoch <- 1 to 10 do
        println (s"Improvement step $epoch")

        // forward prop: input -> hidden
        u  = x * a + Œ±                                          // hidden pre-activation matrix
        z  = f_sigmoid.fM (u)                                   // hidden matrix from f0 activation

        // forward prop: hidden -> output
        v  = z * b + Œ≤                                          // output pre-activation matrix
        ≈∑  = f_sigmoid.fM (v)                                   // output/prediction matrix from f1 activation
        Œµ  = y - ≈∑                                              // error matrix

        // backward prop: hidden <- output
        ∆í1 = ≈∑ ‚äô (1.0 - ≈∑)                                      // derivative (f1') for sigmoid
        Œ¥1 = -Œµ ‚äô ∆í1                                            // delta correction matrix via Hadamard product
        g1 = z.ùêì * Œ¥1                                           // transposed Jacobian matrix (gradients)

        // backward prop: input <- hidden
        ∆í0 = z ‚äô (1.0 - z)                                      // derivative (f0') for sigmoid
        Œ¥0 = (Œ¥1 * b.ùêì) ‚äô ∆í0                                    // delta correction matrix
        g0 = x.ùêì * Œ¥0                                           // transposed Jacobian matrix (gradients)

        // parameter updates
        b -= g1 * Œ∑                                             // update output parameter/weight matrix
        Œ≤ -= Œ¥1.mean * Œ∑                                        // update output bias vector
        a -= g0 * Œ∑                                             // update hidden parameter/weight matrix
        Œ± -= Œ¥0.mean * Œ∑                                        // update hidden bias vector
        val sse = Œµ.normSq                                      // sum of squared errors
        val r2  = -(sse / sst - 1.0)                            // R^2 (recoded to avoid Ops clash)

        println (s"""
        u  = $u
        z  = $z
        v  = $v
        ≈∑  = $≈∑
        Œµ  = $Œµ
        ∆í1 = $∆í1
        Œ¥1 = $Œ¥1
        g1 = $g1
        ∆í0 = $∆í0
        Œ¥0 = $Œ¥0
        g0 = $g0
        b  = $b
        Œ≤  = $Œ≤
        a  = $a
        Œ±  = $Œ±
        r2 = $r2
        """)
    end for

    new Plot (null, y(?, 0), ≈∑(?, 0), "GD for Three-layer Neural Net y_0", lines = true)
    new Plot (null, y(?, 1), ≈∑(?, 1), "GD for Three-layer Neural Net y_1", lines = true)

end simpleNN6


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleNN7` main function illustrates the use of Incremental Gradient Descent (IGD)
 *  to optimize the weights/parameters of a simple neural network (3-layer (1 hidden) Neural Network).
 *
 *  Prediction Equation: z = f0(A^T x + Œ±)
 *                       ≈∑ = f1(B^T z + Œ≤)
 *
 *  where x is an input vector, z is the hidden layer vector, ≈∑ is a predicted output vector, f0, f1
 *  are the activation functions, A and B are the parameter matrices, and Œ± and Œ≤ are the bias vectors.
 *  @note: Stochastic Gradient Descent (SGD) adds stochastic selection to IGD.  In practice,
 *  mini-batches of size 32, 64, or 128 are commonly used.
 *  Computations done at the vector level, x -> z -> y.  R^2 = .768, .869 (for 10 epochs)
 *  > runMain scalation.modeling.simpleNN7
 */
@main def simpleNN7 (): Unit =

    import scalation.mathstat.MatrixDOps.‚äó                      // outer product of two vectors: v1 ‚äó v2 = v1 v1.ùêì
                                                                // matrix where m_ij = v1_i * v2_j

    val (xx, yy) = (xy(?, 1 until 3), xy(?, 3 until 5))         // input matrix, output/response matrix
    val sst    = (yy - yy.mean).normSq                          // sum of squares total, per column
    val a      = MatrixD ((2, 3),  0.2,  0.3,  0.2,             // parameter/weight matrix: input -> hidden
                                  -0.1, -0.2, -0.1)
    val Œ±      = VectorD (-0.1, -0.1, -0.1)                     // hidden layer bias vector
    val b      = MatrixD ((3, 2), 0.1, -0.1,                    // parameter/weight matrix: hidden -> output
                                  0.2, -0.2,
                                  0.1, -0.1)                    // initial weights/parameters (random in practice)
    val Œ≤      = VectorD (-0.1, 0.1)                            // output layer bias vector

    println (s"sst = $sst, xx = $xx, yy = $yy")

    val Œ∑ = 20.0                                                // learning rate (to be tuned)
    var u, z, v, ≈∑, Œµ, ∆í1, Œ¥1, ∆í0, Œ¥0: VectorD = null
    var g1, g0: MatrixD = null
    val yp = new MatrixD (yy.dim, yy.dim2)                      // save each prediction in yp

    for epoch <- 1 to 1 do
        println (s"Improvement step $epoch")
        val sse = new VectorD (2)
        for i <- xx.indices do
            val (x, y) = (xx(i), yy(i))                         // randomize i for Stochastic Gradient Descent (SGD)

            // forward prop: input -> hidden
            u  = a.ùêì * x + Œ±                                    // hidden pre-activation vector
            z  = sigmoid_ (u)                                   // hidden vector from f0 activation

            // forward prop: hidden -> output
            v  = b.ùêì * z + Œ≤                                    // output pre-activation vector
            ≈∑  = sigmoid_ (v)                                   // output/prediction vector from f1 activation
            Œµ  = y - ≈∑                                          // error vector [Œµ_1, Œµ_2]

            // backward prop: hidden <- output
            ∆í1 = ≈∑ * (1.0 - ≈∑)                                  // derivative (f1') for sigmoid
            Œ¥1 = -Œµ * ∆í1                                        // delta correction vector via elementwise product
            g1 = z ‚äó Œ¥1                                         // transposed Jacobian matrix (gradients)

            // backward prop: input <- hidden
            ∆í0 = z * (1.0 - z)                                  // derivative (f0') for sigmoid
            Œ¥0 = b * Œ¥1 * ∆í0                                    // delta correction vector
            g0 = x ‚äó Œ¥0                                         // transposed Jacobian matrix (gradients)

            // parameter updates
            b -= g1 * Œ∑                                         // update output parameter/weight matrix
            Œ≤ -= Œ¥1 * Œ∑                                         // update output bias vector
            a -= g0 * Œ∑                                         // update hidden parameter/weight matrix
            Œ± -= Œ¥0 * Œ∑                                         // update hidden bias vector
            yp(i) = ≈∑                                           // save i-th prediction
            sse += Œµ * Œµ                                        // sum of squared errors
            val r2 = -(sse / sst - 1.0)                         // R^2 (recoded to avoid Ops clash)

            println (s"""
            u  = $u
            z  = $z
            v  = $v
            ≈∑  = $≈∑
            Œµ  = $Œµ
            ∆í1 = $∆í1
            Œ¥1 = $Œ¥1
            g1 = $g1
            ∆í0 = $∆í0
            Œ¥0 = $Œ¥0
            g0 = $g0
            b  = $b
            Œ≤  = $Œ≤
            a  = $a
            Œ±  = $Œ±
            sse= $sse
            r2 = $r2
            """)
        end for
    end for

    new Plot (null, yy(?, 0), yp(?, 0), "IGD for Three-layer Neural Net y_0", lines = true)
    new Plot (null, yy(?, 1), yp(?, 1), "IGD for Three-layer Neural Net y_1", lines = true)

end simpleNN7


//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `simpleNN8` main function illustrates the use of Incremental Gradient Descent (IGD)
 *  to optimize the weights/parameters of a simple neural network (3-layer (1 hidden) Neural Network).
 *
 *  Prediction Equation: z = f0(A^T x + Œ±)
 *                       ≈∑ = f1(B^T z + Œ≤)
 *
 *  where x is an input vector, z is the hidden layer vector, ≈∑ is a predicted output vector, f0, f1
 *  are the activation functions, A and B are the parameter matrices, and Œ± and Œ≤ are the bias vectors.
 *  Illustrates the need for RESCALING the data.
 *  Computations done at the vector level, x -> z -> y.  R^2 = .512 (no rescaling) .813 (rescaling) (for 20 epochs)
 *  > runMain scalation.modeling.simpleNN8
 */
@main def simpleNN8 (): Unit =


    import Example_AutoMPG.xy
    import scalation.mathstat.MatrixDOps.‚äó                      // outer product of two vectors: v1 ‚äó v2 = v1 v1.ùêì
                                                                // matrix where m_ij = v1_i * v2_j
    def rescale (x: MatrixD, yes: Boolean): Unit =
        if yes then
            for j <- x.indices2 do
                val (mu_j, sig_j) = (x(?, j).mean, x(?, j).stdev)
                x(?, j) = (x(?, j) - mu_j) / sig_j              // option: subtract mean, divide by standard deviation
    end rescale
    val n       = xy.dim2 - 1                                   // last column in xy
    val (xx, y) = (xy.not(?, n), xy(?, n))                      // (data/input matrix, response column)
    val yy      = MatrixD.fromVector (y)                        // turn the m-vector y into an m-by-1 matrix

    val sst     = (yy - yy.mean).normSq                         // sum of squares total, per column
    val a       = MatrixD.fill (6, 3, 0.1)                      // parameter/weight matrix: input -> hidden
    val Œ±       = VectorD (0.1, 0.1, 0.1)                       // hidden layer bias vector
    val b       = MatrixD ((3, 1), 0.1,                         // parameter/weight matrix: hidden -> output
                                   0.1,
                                   0.1)                         // initial weights/parameters (random in practice)
    val Œ≤       = VectorD (0.1)                                 // output layer bias vector

    println (s"sst = $sst")
    rescale (xx, true)                                          // true => rescale, false => no rescaling
    println (s"xx = $xx")

    val Œ∑ = 0.1                                                 // learning rate (to be tuned)
    var x, u, z, v, ≈∑, Œµ, ∆í1, Œ¥1, ∆í0, Œ¥0: VectorD = null
    var g1, g0: MatrixD = null
    val yp = new MatrixD (yy.dim, yy.dim2)                      // save each prediction in yp

    for epoch <- 1 to 20 do
        println (s"Improvement step $epoch")
        val sse = new VectorD (1)
        for i <- xx.indices do
            x     = xx(i)                                       // randomize i for Stochastic Gradient Descent (SGD)
            val y = yy(i)

            // forward prop: input -> hidden
            u  = a.ùêì * x + Œ±                                    // hidden pre-activation vector
            z  = sigmoid_ (u)                                   // hidden vector from f0 = sigmoid activation

            // forward prop: hidden -> output
            v  = b.ùêì * z + Œ≤                                    // output pre-activation vector
            ≈∑  = v                                              // output/prediction vector from f1 = id activation
            Œµ  = y - ≈∑                                          // error vector [Œµ_1, Œµ_2]

            // backward prop: hidden <- output
            ∆í1 = VectorD (1.0)                                  // derivative (f1') for id
            Œ¥1 = -Œµ * ∆í1                                        // delta correction vector via elementwise product
            g1 = z ‚äó Œ¥1                                         // transposed Jacobian matrix (gradients)

            // backward prop: input <- hidden
            ∆í0 = z * (1.0 - z)                                  // derivative (f0') for sigmoid
            Œ¥0 = b * Œ¥1 * ∆í0                                    // delta correction vector
            g0 = x ‚äó Œ¥0                                         // transposed Jacobian matrix (gradients)

            // parameter updates
            b -= g1 * Œ∑                                         // update output parameter/weight matrix
            Œ≤ -= Œ¥1 * Œ∑                                         // update output bias vector
            a -= g0 * Œ∑                                         // update hidden parameter/weight matrix
            Œ± -= Œ¥0 * Œ∑                                         // update hidden bias vector
            yp(i) = ≈∑                                           // save i-th prediction
            sse += Œµ * Œµ                                        // sum of squared errors
        end for
        val r2 = -(sse / sst - 1.0)                         // R^2 (recoded to avoid Ops clash)

        println (s"""
        x  = $x
        u  = $u
        z  = $z
        v  = $v
        ≈∑  = $≈∑
        Œµ  = $Œµ
        ∆í1 = $∆í1
        Œ¥1 = $Œ¥1
        g1 = $g1
        ∆í0 = $∆í0
        Œ¥0 = $Œ¥0
        g0 = $g0
        b  = $b
        Œ≤  = $Œ≤
        a  = $a
        Œ±  = $Œ±
        sse= $sse
        r2 = $r2
        """)
    end for

    new Plot (null, yy(?, 0), yp(?, 0), "IGD for Three-layer Neural Net y_0", lines = true)

end simpleNN8

